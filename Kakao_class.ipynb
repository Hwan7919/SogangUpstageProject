{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "# transformers: 다양한 사전 학습된 모델을 제공하는 라이브러리\n",
        "# sentencepiece: 텍스트 토크나이징을 위한 라이브러리\n",
        "# onnxruntime: ONNX 모델을 실행하기 위한 런타임\n",
        "# gradio: 웹 인터페이스를 쉽게 만들 수 있는 라이브러리\n",
        "!pip install -q transformers sentencepiece onnxruntime gradio\n",
        "\n",
        "# onnx 패키지 설치\n",
        "# ONNX(Open Neural Network Exchange) 모델을 다루기 위한 패키지\n",
        "!pip install onnx\n",
        "\n",
        "# 필요한 모듈 임포트\n",
        "import re  # 정규 표현식 모듈\n",
        "import torch  # PyTorch 라이브러리\n",
        "import onnxruntime as ort  # ONNX 런타임\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification  # 트랜스포머 모델과 토크나이저\n",
        "import numpy as np  # 수치 계산 라이브러리\n",
        "import pandas as pd  # 데이터프레임 처리 라이브러리\n",
        "import gradio as gr  # 웹 인터페이스 라이브러리\n",
        "from openai import OpenAI  # OpenAI API 사용을 위한 라이브러리"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M3yTbahQFrv",
        "outputId": "c4565750-ec56-4f0c-e872-8c483f481509"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT 모델과 토크나이저 로드\n",
        "# \"monologg/kobert\" 사전 학습된 KoBERT 모델과 해당 토크나이저를 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"monologg/kobert\", num_labels=3)\n",
        "\n",
        "# ONNX 모델 저장 경로 설정\n",
        "onnx_model_path = \"kobert_sentiment.onnx\"\n",
        "\n",
        "# PyTorch 모델을 ONNX 형식으로 변환 및 저장\n",
        "torch.onnx.export(\n",
        "    model,  # 변환할 모델\n",
        "    (torch.tensor([[1]]), torch.tensor([[1]])),  # 더미 입력 (input_ids, attention_mask)\n",
        "    onnx_model_path,  # 저장할 파일 경로\n",
        "    input_names=[\"input_ids\", \"attention_mask\"],  # 입력 이름 지정\n",
        "    output_names=[\"logits\"],  # 출력 이름 지정\n",
        "    dynamic_axes={\n",
        "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},  # 동적 배치 크기 및 시퀀스 길이\n",
        "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "        \"logits\": {0: \"batch_size\"}\n",
        "    },\n",
        "    opset_version=14  # ONNX opset 버전 지정\n",
        ")\n",
        "\n",
        "# ONNX 모델 로드\n",
        "# 변환된 ONNX 모델을 InferenceSession으로 로드하여 추론 준비\n",
        "ort_session = ort.InferenceSession(onnx_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1u19Vy7SCOv",
        "outputId": "01144c97-fb3a-4dbf-b4b0-3fceb5ece91d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for monologg/kobert contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/monologg/kobert.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] ㅛ\n",
            "The repository for monologg/kobert contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/monologg/kobert.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment_onnx(text):\n",
        "    \"\"\"\n",
        "    주어진 텍스트의 감정을 예측하는 함수.\n",
        "\n",
        "    Args:\n",
        "        text (str): 분석할 텍스트.\n",
        "\n",
        "    Returns:\n",
        "        tuple: 예측된 레이블(정수)과 해당 레이블의 신뢰도.\n",
        "    \"\"\"\n",
        "    # 텍스트를 토크나이즈하여 NumPy 배열 형태로 변환\n",
        "    inputs = tokenizer(text, return_tensors=\"np\", padding=True, truncation=True, max_length=128)\n",
        "    ort_inputs = {\n",
        "        \"input_ids\": inputs[\"input_ids\"],  # 입력 토큰 IDs\n",
        "        \"attention_mask\": inputs[\"attention_mask\"]  # 어텐션 마스크\n",
        "    }\n",
        "    # ONNX 모델을 사용하여 로짓(logits) 계산\n",
        "    logits = ort_session.run(None, ort_inputs)[0]\n",
        "    # 소프트맥스를 적용하여 확률로 변환\n",
        "    probabilities = torch.nn.functional.softmax(torch.from_numpy(logits), dim=-1).detach().numpy()\n",
        "    # 가장 높은 확률을 가진 레이블 선택\n",
        "    predicted_label = np.argmax(probabilities, axis=1)[0]\n",
        "    # 해당 레이블의 신뢰도 추출\n",
        "    confidence = probabilities[0][predicted_label]\n",
        "    return predicted_label, confidence"
      ],
      "metadata": {
        "id": "pi0Qr1TySCRF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_affinity_score(messages):\n",
        "    \"\"\"\n",
        "    메시지 리스트를 기반으로 호감도 점수를 계산하는 함수.\n",
        "\n",
        "    Args:\n",
        "        messages (list): 각 메시지에 대한 정보(감정 및 신뢰도)를 포함한 딕셔너리 리스트.\n",
        "\n",
        "    Returns:\n",
        "        float: 0부터 100 사이의 호감도 점수.\n",
        "    \"\"\"\n",
        "    score = 0  # 초기 점수\n",
        "    # 감정에 따른 가중치 설정\n",
        "    sentiment_weights = {\n",
        "        \"긍정\": 1.0,\n",
        "        \"중립\": 0.0,\n",
        "        \"부정\": -1.0\n",
        "    }\n",
        "    # 각 메시지의 감정에 따라 점수 계산\n",
        "    for msg in messages:\n",
        "        weight = sentiment_weights.get(msg[\"sentiment\"], 0)  # 감정에 따른 가중치\n",
        "        score += weight * msg[\"confidence\"]  # 가중치와 신뢰도를 곱하여 점수에 추가\n",
        "    # 전체 점수를 0에서 100 사이로 정규화\n",
        "    total_score = (score / len(messages)) * 50 + 50\n",
        "    return max(0, min(total_score, 100))  # 점수가 0보다 작거나 100보다 크지 않도록 제한"
      ],
      "metadata": {
        "id": "OEYAch_3SCTM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_conversation(file_path, api_key):\n",
        "    \"\"\"\n",
        "    대화 파일을 분석하여 요약, 호감도 점수, 평가, 조언을 제공하는 함수.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): 분석할 대화 파일의 경로 (CSV 형식).\n",
        "        api_key (str): Upstage API 키.\n",
        "\n",
        "    Returns:\n",
        "        tuple: 대화 요약, 호감도 점수, 호감도 평가, 조언.\n",
        "    \"\"\"\n",
        "    # 메시지 전처리\n",
        "    # CSV 파일을 읽어와 메시지 리스트 생성\n",
        "    df = pd.read_csv(file_path, delimiter=\",\", encoding='utf-8')\n",
        "    messages = [{\"time\": row['Date'], \"sender\": row['User'], \"message\": row['Message']} for _, row in df.iterrows()]\n",
        "\n",
        "    # 감정 분석\n",
        "    sentiment_labels = {0: \"부정\", 1: \"중립\", 2: \"긍정\"}  # 레이블과 감정 매핑\n",
        "    for msg in messages:\n",
        "        label, confidence = predict_sentiment_onnx(msg[\"message\"])  # 감정 예측\n",
        "        msg[\"sentiment\"] = sentiment_labels.get(label, \"알 수 없음\")  # 감정 레이블 추가\n",
        "        msg[\"confidence\"] = confidence  # 신뢰도 추가\n",
        "\n",
        "    # 호감도 점수 계산\n",
        "    affinity_score = calculate_affinity_score(messages)\n",
        "\n",
        "    # 호감도 평가\n",
        "    if 0 <= affinity_score <= 20:\n",
        "        evaluation = \"강한 부정\"\n",
        "    elif 21 <= affinity_score <= 40:\n",
        "        evaluation = \"약한 부정\"\n",
        "    elif 41 <= affinity_score <= 60:\n",
        "        evaluation = \"중립\"\n",
        "    elif 61 <= affinity_score <= 80:\n",
        "        evaluation = \"약한 긍정\"\n",
        "    elif 81 <= affinity_score <= 100:\n",
        "        evaluation = \"강한 긍정\"\n",
        "\n",
        "    # Upstage 솔라 API 설정\n",
        "    # OpenAI API 클라이언트를 Upstage 솔라 엔드포인트로 초기화\n",
        "    client = OpenAI(api_key=api_key, base_url=\"https://api.upstage.ai/v1/solar\")\n",
        "\n",
        "    # 대화 요약\n",
        "    # 모든 메시지를 하나의 텍스트로 결합\n",
        "    conversation_text = \"\\n\".join([f\"{msg['sender']}: {msg['message']}\" for msg in messages])\n",
        "    summary_prompt = (\n",
        "        \"아래는 두 사람 간의 대화 내용입니다. 이 대화를 요약하여 주요 주제, 상대방의 관심사, 감정 등을 파악해주세요:\\n\\n\"\n",
        "        f\"{conversation_text}\\n\\n요약:\"\n",
        "    )\n",
        "    # 요약 생성 요청\n",
        "    summary_result = client.chat.completions.create(\n",
        "        model=\"solar-1-mini-chat\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"당신은 전문 요약가입니다.\"},\n",
        "            {\"role\": \"user\", \"content\": summary_prompt}\n",
        "        ]\n",
        "    )\n",
        "    # 요약된 내용 추출\n",
        "    conversation_summary = summary_result.choices[0].message.content.strip()\n",
        "\n",
        "    # 조언 생성\n",
        "    prompt = (\n",
        "        f\"아래는 두 사람 간의 대화 요약입니다:\\n\\n{conversation_summary}\\n\\n\"\n",
        "        f\"대화 상대의 호감도 점수가 {affinity_score:.2f}점이며, 평가 결과는 '{evaluation}'입니다. \"\n",
        "        f\"이 요약된 대화를 바탕으로 상대방의 성향을 분석하고, 그에 따른 맞춤형 조언을 제공해주세요.\"\n",
        "    )\n",
        "    # 조언 생성 요청\n",
        "    advice_result = client.chat.completions.create(\n",
        "        model=\"solar-1-mini-chat\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"당신은 연애 상담 전문가입니다.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    # 조언 내용 추출\n",
        "    advice = advice_result.choices[0].message.content.strip()\n",
        "\n",
        "    return conversation_summary, f\"{affinity_score:.2f}\", evaluation, advice"
      ],
      "metadata": {
        "id": "1L2KZTgtSCWU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio 인터페이스 정의\n",
        "# 사용자로부터 입력을 받고 결과를 출력하는 웹 인터페이스 구성\n",
        "gr.Interface(\n",
        "    fn=analyze_conversation,  # 호출할 함수\n",
        "    inputs=[\n",
        "        gr.File(label=\"카카오톡 채팅 파일 (CSV)\"),  # CSV 파일 업로드 입력\n",
        "        gr.Textbox(label=\"Upstage API Key\", type=\"password\")  # API 키 입력 (비밀번호 형태)\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"대화 요약\"),  # 대화 요약 출력\n",
        "        gr.Textbox(label=\"호감도 점수\"),  # 호감도 점수 출력\n",
        "        gr.Textbox(label=\"호감도 평가\"),  # 호감도 평가 출력\n",
        "        gr.Textbox(label=\"조언\")  # 조언 출력\n",
        "    ],\n",
        "    title=\"카카오톡 대화 분석 및 호감도 평가\"  # 인터페이스 제목\n",
        ").launch()  # 웹 인터페이스 실행"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "H9bbcgQtSCbw",
        "outputId": "c6411849-a5ba-4ebb-ec7c-f9c662677ec0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2af337d2084cf3c489.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2af337d2084cf3c489.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3nzhYPbSCd1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drMHf3pQQFtU"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}